import torch
from env.drone_env import DroneEnv
from agents.ddpg_agent import DIRECTIONS

def run_warmup_on_env(agent, steps=500):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç warm-up: –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç N –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∏–∑ –æ–¥–Ω–æ–π —Å—Ä–µ–¥—ã –≤ –±—É—Ñ–µ—Ä –∞–≥–µ–Ω—Ç–∞.
    """
    env = DroneEnv()
    obs, _ = env.reset()

    for _ in range(steps):
        action_idx = env.action_space.sample()
        move_direction = DIRECTIONS[action_idx]
        action_tensor = torch.tensor(move_direction, dtype=torch.float32)

        next_obs, reward, terminated, truncated, _ = env.step(move_direction)
        done = terminated or truncated

        agent.replay_buffer.add(obs, action_tensor.numpy(), reward, next_obs, float(done))
        obs = next_obs if not done else env.reset()[0]

    print(f"‚úÖ Warm-up: {steps} transitions collected from one env.")


def generate_warmup_experience(agent, num_envs=50, steps_per_env=120):
    """
    –°–æ–∑–¥–∞—ë—Ç num_envs —Å—Ä–µ–¥ (–ø–æ –æ–¥–Ω–æ–π –∑–∞ —Ä–∞–∑) –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç warm-up –Ω–∞ –∫–∞–∂–¥–æ–π.
    –í—Å–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ –æ–¥–∏–Ω –±—É—Ñ–µ—Ä –∞–≥–µ–Ω—Ç–∞.
    """
    total_transitions = 0

    for i in range(num_envs):
        print(f"‚ñ∂ Warm-up env {i}...")
        run_warmup_on_env(agent, steps=steps_per_env)
        total_transitions += steps_per_env

    print(f"\nüèÅ Warm-up complete: {total_transitions} transitions collected from {num_envs} environments.")
